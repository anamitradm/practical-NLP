{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKx3ryPiqp_B"
      },
      "source": [
        "# Capstone: Learning to Rank\n",
        "## RankNet\n",
        "\n",
        "In this notebook we will train on random data the learning to rank model RankNet.\n",
        "\n",
        "The idea behind LTR is always to start with a dataset of some queries, their returned documents and the score of relevance. This relevance may be an *a posteriori* metric like number of clicks.\n",
        "\n",
        "You can run this lab both locally or in Colab.\n",
        "\n",
        "- To run in Colab just go to `https://colab.research.google.com`, sign-in and you upload this notebook. Colab has GPU access for free.\n",
        "- To run locally just run `jupyter notebook` and access the notebook in this lab. You would need to first install the requirements in `requirements.txt`\n",
        "\n",
        "Follow the instructions. Good luck!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e01Zw2-rqp_F"
      },
      "source": [
        "The idea behind RankNet is to model the **joint probability** that `document i` comes before `document j` as the following:\n",
        "\n",
        "$P_{ij} = 1$ if $s_i > s_j$\n",
        "$P_{ij} = 0.5$ if $s_i = s_j$\n",
        "$P_{ij} = 0$ if $s_i < s_j$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31h0sZUWqp_F"
      },
      "source": [
        "So for *every pair of inputs* we will calculate both outputs, substract them, pass a logistic function to model the probability:\n",
        "\n",
        "<img src=\"https://github.com/axel-sirota/practical-nlp/blob/main/1-similarity/ranknet.png?raw=1\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bb6KgZcudKT",
        "outputId": "833d66a5-0b37-4ad4-d7b6-690d607d3b18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct 20 18:32:12 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVyuAnDLyvBE",
        "outputId": "d4f502ee-a107-440c-9385-2c773f7838dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==4.2.0\n",
            "  Downloading gensim-4.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.2.0) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.2.0) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim==4.2.0) (1.21.6)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install 'gensim==4.2.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aLsDbhgoqZyt"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras import layers, activations, losses, Model, Input\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from textblob import TextBlob, Word\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import Constant\n",
        "from tensorflow.nn import leaky_relu\n",
        "from tensorflow.keras.utils import plot_model, Progbar\n",
        "from gensim.models import Doc2Vec\n",
        "import gensim\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import warnings\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from itertools import combinations\n",
        "import matplotlib.pyplot as plt\n",
        "import multiprocessing\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "from itertools import combinations\n",
        "\n",
        "import gensim\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from gensim.models import Doc2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.layers import Activation, Dense, Subtract\n",
        "from tensorflow.nn import leaky_relu\n",
        "\n",
        "TRACE = False\n",
        "embedding_dim = 100\n",
        "epochs=10\n",
        "batch_size = 50\n",
        "sample_queries = 20\n",
        "sample_results_dataset = 100\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RWTlcgNpqZyv"
      },
      "outputs": [],
      "source": [
        "def set_seeds_and_trace():\n",
        "  os.environ['PYTHONHASHSEED'] = '0'\n",
        "  np.random.seed(42)\n",
        "  tf.random.set_seed(42)\n",
        "  random.seed(42)\n",
        "  if TRACE:\n",
        "    tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "def set_session_with_gpus_and_cores():\n",
        "  cores = multiprocessing.cpu_count()\n",
        "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
        "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "  sess = tf.compat.v1.Session(config=config) \n",
        "  K.set_session(sess)\n",
        "\n",
        "set_seeds_and_trace()\n",
        "set_session_with_gpus_and_cores()\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVRPlgP2wcpc",
        "outputId": "5b99096d-342d-4747-964e-ae4baba17d49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing get_data.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile get_data.sh\n",
        "\n",
        "if [ ! -f yelp.csv ]; then\n",
        "  wget -O yelp.csv https://www.dropbox.com/s/xds4lua69b7okw8/yelp.csv?dl=0\n",
        "fi\n",
        "if [ ! -f doc2vec_yelp_model ]; then\n",
        "  wget -O doc2vec_yelp_model https://www.dropbox.com/s/bibu9bashb0cd68/doc2vec_yelp_model?dl=0\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_HceAKmwc3m",
        "outputId": "19a5522c-a506-4b9a-d2cb-957a0e2f4acc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-20 20:30:24--  https://www.dropbox.com/s/xds4lua69b7okw8/yelp.csv?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/xds4lua69b7okw8/yelp.csv [following]\n",
            "--2022-10-20 20:30:24--  https://www.dropbox.com/s/raw/xds4lua69b7okw8/yelp.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc9da2e1b518a7e0873699e20fcb.dl.dropboxusercontent.com/cd/0/inline/BvN-ew-xwQ6-3lnQHUaFL5147xpFBsYHhEyrWKMC3OcfhKxNUSnSkMFuhGEu3mxHTIvuxFDJ5WRoIFgZrSn3Rj59x-iK2uxkSOKDHspYsXA0ndgEudgOjiPBVKCetE3-JIqr-BMNbx3onIYNQqO1FEUizJQnPFz7yg9Gj67ZA4A5Jg/file# [following]\n",
            "--2022-10-20 20:30:25--  https://uc9da2e1b518a7e0873699e20fcb.dl.dropboxusercontent.com/cd/0/inline/BvN-ew-xwQ6-3lnQHUaFL5147xpFBsYHhEyrWKMC3OcfhKxNUSnSkMFuhGEu3mxHTIvuxFDJ5WRoIFgZrSn3Rj59x-iK2uxkSOKDHspYsXA0ndgEudgOjiPBVKCetE3-JIqr-BMNbx3onIYNQqO1FEUizJQnPFz7yg9Gj67ZA4A5Jg/file\n",
            "Resolving uc9da2e1b518a7e0873699e20fcb.dl.dropboxusercontent.com (uc9da2e1b518a7e0873699e20fcb.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to uc9da2e1b518a7e0873699e20fcb.dl.dropboxusercontent.com (uc9da2e1b518a7e0873699e20fcb.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8091185 (7.7M) [text/plain]\n",
            "Saving to: ‘yelp.csv’\n",
            "\n",
            "yelp.csv            100%[===================>]   7.72M  50.5MB/s    in 0.2s    \n",
            "\n",
            "2022-10-20 20:30:25 (50.5 MB/s) - ‘yelp.csv’ saved [8091185/8091185]\n",
            "\n",
            "--2022-10-20 20:30:25--  https://www.dropbox.com/s/bibu9bashb0cd68/doc2vec_yelp_model?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/bibu9bashb0cd68/doc2vec_yelp_model [following]\n",
            "--2022-10-20 20:30:26--  https://www.dropbox.com/s/raw/bibu9bashb0cd68/doc2vec_yelp_model\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucdd5185633af1101b3c26500927.dl.dropboxusercontent.com/cd/0/inline/BvOjXD3xVCCnqUl5kU6k8U6Js7Ck1bPAX_kN6nKgNiof-iX-gL7dizPLBQ9FokSgkP6HSnvJFVNr1YZ9m4ahQG-ibfNTZ5-Oh76Stam10qYmcPvEbcTbQpw5wUXxKCN_fxQabv0uzkD1XhuBRpKdKPq_Y1tioBQVLcwhiXMEH7KMPg/file# [following]\n",
            "--2022-10-20 20:30:26--  https://ucdd5185633af1101b3c26500927.dl.dropboxusercontent.com/cd/0/inline/BvOjXD3xVCCnqUl5kU6k8U6Js7Ck1bPAX_kN6nKgNiof-iX-gL7dizPLBQ9FokSgkP6HSnvJFVNr1YZ9m4ahQG-ibfNTZ5-Oh76Stam10qYmcPvEbcTbQpw5wUXxKCN_fxQabv0uzkD1XhuBRpKdKPq_Y1tioBQVLcwhiXMEH7KMPg/file\n",
            "Resolving ucdd5185633af1101b3c26500927.dl.dropboxusercontent.com (ucdd5185633af1101b3c26500927.dl.dropboxusercontent.com)... 162.125.80.15, 2620:100:601b:15::a27d:80f\n",
            "Connecting to ucdd5185633af1101b3c26500927.dl.dropboxusercontent.com (ucdd5185633af1101b3c26500927.dl.dropboxusercontent.com)|162.125.80.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BvNtYNUzb8-vMJza9GrXkcedco0-kgMaNErQBZ8NqIlYaWZbrH5IKBqUqrz_l1qySp7oTcBAlbPnOKPGt7-cDBeZKjxeharRUEhx1MPYPhm9akmBaJKHN6qaHujrBkB2SGFXEQ9R1CtjF18eqnbE5SYRr5zso4WptN0gJ6n1154hOWfRaeyrm1SPei4eCI-hs-CKFa-70DLof2oAZOM0mQkWr-A-CwHBwAp3N70nE4BMm-kAQmzIMoFvp5IdmHDKo_XJIS8BBT64nI1eabTi1_q4XSyUlG3uAu2b02vrzibG20qMa8f2Gc52b47xQuz7MDVHtNKzQyOwIcDApmnsMKMHW7Ew_qcTBrsYOPdjW9Jc22N4CTPNABwh8E8sVfkdKrWQBec5WFZ0ZMUAWvvxCnvn3cbBRrij9-Qbqufrmvw37Q/file [following]\n",
            "--2022-10-20 20:30:27--  https://ucdd5185633af1101b3c26500927.dl.dropboxusercontent.com/cd/0/inline2/BvNtYNUzb8-vMJza9GrXkcedco0-kgMaNErQBZ8NqIlYaWZbrH5IKBqUqrz_l1qySp7oTcBAlbPnOKPGt7-cDBeZKjxeharRUEhx1MPYPhm9akmBaJKHN6qaHujrBkB2SGFXEQ9R1CtjF18eqnbE5SYRr5zso4WptN0gJ6n1154hOWfRaeyrm1SPei4eCI-hs-CKFa-70DLof2oAZOM0mQkWr-A-CwHBwAp3N70nE4BMm-kAQmzIMoFvp5IdmHDKo_XJIS8BBT64nI1eabTi1_q4XSyUlG3uAu2b02vrzibG20qMa8f2Gc52b47xQuz7MDVHtNKzQyOwIcDApmnsMKMHW7Ew_qcTBrsYOPdjW9Jc22N4CTPNABwh8E8sVfkdKrWQBec5WFZ0ZMUAWvvxCnvn3cbBRrij9-Qbqufrmvw37Q/file\n",
            "Reusing existing connection to ucdd5185633af1101b3c26500927.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13243389 (13M) [application/octet-stream]\n",
            "Saving to: ‘doc2vec_yelp_model’\n",
            "\n",
            "doc2vec_yelp_model  100%[===================>]  12.63M  6.57MB/s    in 1.9s    \n",
            "\n",
            "2022-10-20 20:30:29 (6.57 MB/s) - ‘doc2vec_yelp_model’ saved [13243389/13243389]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!bash get_data.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ec69pypxwc6Z"
      },
      "outputs": [],
      "source": [
        "model = Doc2Vec.load(\"./doc2vec_yelp_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mHwqq3Vdwc9k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e356ec8a-7505-4e79-e77d-aaf33bfb24b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0     I really try to like Old Town Scottsdale - and...\n",
            "1     (aka. SKETCHY TEMPE with BONNIE G, Part One of...\n",
            "2     Thought Saturday night would be busy at 6:00 P...\n",
            "3     I was actually really impressed, even though I...\n",
            "4     Pros:\\n1.  Excellent service.  Hell, it's damn...\n",
            "5     Treated with complete disrespect. Worst servic...\n",
            "6     I met up with a girlfriend at borders. This Bo...\n",
            "7     First time here and it was really good. I orde...\n",
            "8     I'm on a low carb diet right now, so I had to ...\n",
            "9     I got my nails done there last Thursday for th...\n",
            "10    I was referred to Jones Family Dentistry for a...\n",
            "11    Just went to this theater last night, and it w...\n",
            "12    The Harkins Camelview 5 gives Arizonans the un...\n",
            "13    Great new addition to the Old Town neighborhoo...\n",
            "14    What can I say that hasn't already been said a...\n",
            "15    This place is essentially a copy of the old Fa...\n",
            "16    I have been coming here since discovering them...\n",
            "17    Delicious barbecue, we had 4 meats platter tha...\n",
            "18    My favorite trail in South Mountain starts at ...\n",
            "19    Great product! I was on a mission to make home...\n",
            "Name: text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "path = './yelp.csv'\n",
        "yelp = pd.read_csv(path)\n",
        "train_set_reviews = yelp.sample(n=sample_results_dataset).reset_index(drop=True)\n",
        "\n",
        "queries = yelp.text.sample(n=sample_queries).reset_index(drop=True)\n",
        "print(queries)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sz8kOxJRwdAm"
      },
      "outputs": [],
      "source": [
        "results = np.zeros((len(queries), len(train_set_reviews), 100))\n",
        "scores = np.zeros((len(queries), len(train_set_reviews)))\n",
        "# the feature vector of the review using the doc2vec model, and the score as the similarity\n",
        "model = gensim.models.doc2vec.Doc2Vec(vector_size=embedding_dim, min_count=2, epochs=3, workers=5)\n",
        "def read_corpus(query, tokens_only=False):\n",
        "       for  line in query:\n",
        "            print(line)\n",
        "            tokens = list(gensim.utils.simple_preprocess(line))  # tokenize and preprocess line. Try to search in gensim\n",
        "            inferred_vector = model.infer_vector(tokens)\n",
        "            print(inferred_vector)\n",
        "                for review in sample_reviews:\n",
        "                  try:\n",
        "                     similarity = model.similarity_unseen_docs(doc_words1= list(gensim.utils.simple_preprocess(query)), doc_words2= list(gensim.utils.simple_preprocess(review)))\n",
        "                   except KeyError:\n",
        "                        similarity = 0\n",
        "                        similarities.append(similarity)\n",
        "\n",
        "            if tokens_only:\n",
        "                yield tokens\n",
        "            else:\n",
        "                # For training data, add tags and yield the result. The end yielded result should be a TaggedDocument\n",
        "                \n",
        "                yield gensim.models.doc2vec.TaggedDocument(tokens)\n",
        "\n",
        "similarities = []\n",
        "for review in sample_reviews:\n",
        "    try:\n",
        "        similarity = model.similarity_unseen_docs(doc_words1= list(gensim.utils.simple_preprocess(query)), doc_words2= list(gensim.utils.simple_preprocess(review)))\n",
        "    except KeyError:\n",
        "        similarity = 0\n",
        "    similarities.append(similarity)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WodU_Ep_zXzY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "67154023-0085-44cd-bc3b-13b511cace54"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ebeb99075bc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mpij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Find pij for each q_ix, pair_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mpij\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pij\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'append'"
          ]
        }
      ],
      "source": [
        "# put data into pairs\n",
        "xi = []\n",
        "xj = []\n",
        "pij = []\n",
        "pair_id = []\n",
        "pair_query_id = []\n",
        "\n",
        "# Fill\n",
        "\n",
        "for q_ix, query in enumerate(queries):\n",
        "    for pair_idx in combinations(enumerate(results[q_ix]), 2):\n",
        "        pair_query_id.append(query)\n",
        "        pair_id.append(pair_idx)\n",
        "        ix_i, document_i = pair_idx[0]\n",
        "        ix_j, document_j = pair_idx[1]\n",
        "        xi.append(document_i)\n",
        "        xj.append(document_j)\n",
        "\n",
        "        pij = None  # Find pij for each q_ix, pair_idx\n",
        "        pij.append(_pij)\n",
        "\n",
        "xi = np.array(xi)\n",
        "xj = np.array(xj)\n",
        "pij = np.array(pij)\n",
        "pair_query_id = np.array(pair_query_id)\n",
        "del results\n",
        "del scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i-IcbTQzX19"
      },
      "outputs": [],
      "source": [
        "# FILL\n",
        "\n",
        "# Split xi, xj, pij, and pair_id into train and test sets.\n",
        "# HINT: stratify by pair_query_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NH_8rmrfzX4X"
      },
      "outputs": [],
      "source": [
        "xi_train = tf.constant(xi_train)\n",
        "xi_test = tf.constant(xi_test)\n",
        "xj_train = tf.constant(xj_train)\n",
        "xj_test = tf.constant(xj_test)\n",
        "pij_train = tf.constant(pij_train)\n",
        "pij_test = tf.constant(pij_test)\n",
        "pair_id_train = pair_id_train\n",
        "pair_id_test = pair_id_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hP0hwcIxqZyz",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# Try to create a model with 2 dense layers with leaky_relu as activations. Then a linear dense function and a substract layer.\n",
        "\n",
        "# This time I will leave it blank, but in the parameter oij you should have the output of the substraction.\n",
        "\n",
        "\n",
        "# model architecture\n",
        "class RankNet(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # FILL\n",
        "\n",
        "    def call(self, inputs):\n",
        "        xi, xj = inputs\n",
        "        # FILL\n",
        "        output = layers.Activation('sigmoid')(oij)\n",
        "        return output\n",
        "\n",
        "    def build_graph(self):\n",
        "        x = [Input(shape=(10)), Input(shape=(10))]\n",
        "        return Model(inputs=x, outputs=self.call(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "IIWp7ZNtqZy2",
        "outputId": "704c5ea9-fcdd-403d-fbb9-fb417bbfedc9",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-81208bfa46ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train model using compile with binary_crossentropy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mranknet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRankNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mranknet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mranknet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RankNet' is not defined"
          ]
        }
      ],
      "source": [
        "# train model using compile with binary_crossentropy.\n",
        "ranknet = RankNet()\n",
        "ranknet.compiler(optimizer='adam',  loss='binary_crossentropy')\n",
        "a = ranknet.build_graph()\n",
        "a.summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "-h0NkqU4qp_M"
      },
      "outputs": [],
      "source": [
        "#Train the model, pass as inputs [xi_train, xj_train] and pij_train.\n",
        "history = None # Fill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nElcBh2dB3If"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQOC3drYqZy2",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# function for plotting loss\n",
        "def plot_metrics(train_metric, val_metric=None, metric_name=None, title=None, ylim=5):\n",
        "    plt.title(title)\n",
        "    plt.ylim(0,ylim)\n",
        "    plt.plot(train_metric,color='blue',label=metric_name)\n",
        "    if val_metric is not None: plt.plot(val_metric,color='green',label='val_' + metric_name)\n",
        "    plt.legend(loc=\"upper right\")\n",
        "\n",
        "# plot loss history\n",
        "plot_metrics(history.history['loss'], history.history['val_loss'], \"Loss\", \"Loss\", ylim=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GY6qE2UpqZy3",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "#Test with a a new sample pair of docs to get their associated probability.\n",
        "\n",
        "new_doci = None\n",
        "new_docj = None\n",
        "inputs = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOdqELijBybJ"
      },
      "outputs": [],
      "source": [
        "ranknet(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9Sp4l4AByrh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}